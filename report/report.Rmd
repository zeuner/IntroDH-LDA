---
output:
  pdf_document:
    highlight: tango
    citation_package: biblatex
    number_sections: true
documentclass: scrartcl
bibliography: report.bib
biblio-style: authoryear
header-includes:
  - \usepackage{setspace}
  - \onehalfspacing
fontsize: 12pt
---
```{=latex}
\begin{titlepage}
\centering
\vfill{}
\input{uni_logo.pdf_tex} \par
\vspace{3cm}
{\bfseries\huge Analysing Transnational Relationships in Parliament Debates \par}
\vspace{1cm}
{\large Agoston Volcz, Jonathan Weinmann, Isidor Zeuner \par
March 14. 2021}

\vfill
Fakultät für Mathematik und Informatik \\
Seminar: 10-207-0001 Einführung in die Digital Humanities \\
Lecturer: Thomas Köntges, Ph.D. \\
Semester: WS 2020/2021


\end{titlepage}
\setcounter{page}{1}
\tableofcontents
\clearpage
```
# Introduction

Parliaments usually make corrected versions of their plenary minutes
available for public inspection on the internet. The minutes contain
the speeches of the MPs, reports and debates. These documents are
accessible to everybody, but the large amount of the documents makes
it difficult to index and search. And it is even more difficult to
recognise complex topics, connections and "between the lines"
content. In order to be able to make a meaningful analysis of these
provided documents, preliminary work must be done. The documents must
be downloaded, cleaned up in a way that elements not carrying any
information will be removed. Furthermore, they have to be converted
into a structured format so that the machine can work with them. The
computer must then be trained to "understand" the documents and
produce human readable output. This preliminary work is to be done
with this project.

Analysing parliament debates on European level was the basic idea of
our Project. Away from the topic, it was the interest in text-based
analysis and topic modelling that brought us together as a group. In
the beginning we didn’t have a concrete idea of a specific research
question. We thought it would be useful to have a simpler and clearer
way of giving an insight into the debates of the parliaments, as it is
hardly possible to keep track of the unbelievably large amount of the
minutes of the meetings. And it is even more difficult to deal
specifically with a particular topic within them. Based on this
impulse, the idea came up to look for transnational relations that
emerge in the debates and using Natural Language Processing and LDA
Topic Modelling as a tool. During the implementation of the project,
it became increasingly clear that the focus was shifting. It was no
longer just about answering this one question that had been asked with
the technical tools, but rather to develop a tool with which it is
possible to work on our initial research idea and similar questions
based on it. As well limits of the method became visible and that it
can only cover a part. So, we came up to rephrase the question like
this: To what extent can our technical approach help finding
transnational relations between countries from the text corpora?

During the report, it should be made clear how the tool was developed,
at which points we reached our limits and which solutions we
considered. The first part of the report describes the organisational
process of the project. How did we communicate and share our results?
Decisions which data and which tools we will use will be presented in
this part. The focus will then be on data collection, conversion and
the topic model. It is about building a tool for web-scraping and
adapting it to the different formats offered on the government
websites. The final step was to create the topic model and to analyse
it. The emphasis in our project is more on the technical aspect and
the handling, controlling and processing of the data sets as well as
the formation of the topic model than on the analysis.

# Organisation

The first thing was to set up an infrastructure in which we could
communicate. For this we first used ChaosPad. This made it possible
for everyone to exchange initial ideas. We decided to use the pad for
the more humanities questions and topics and to switch to Git and
Github for the exchange and organisation of code. After setting up a
RocketChat-Channel for organisational purposes, however, most
communication, both organisational, technical and intellectual,
quickly took place there.

Since the topic of the project is the analysis of parliamentary
debates of different but of course as many European countries as
possible we already encountered the first difficulty and above all the
first limitation which would mean a bias in the analysis. In order to
interpret the debates as accurately as possible, it is a prerequisite
that we speak the languages or at least have a certain understanding
of them. So, we ended up working with data from the parliaments of
Austria, Belgium, the Czech Republic, Hungary, Italy, Ireland, Poland,
France and the Netherlands. It is important to emphasise that we are
limited at this point and that this is not an ideal condition. The
parliamentary minutes were made available on the websites of the
respective governments. However, it turned out that both the scope in
terms of years and the type of document as well as the presentation in
the documents varied considerably. The detailed data collection is
dealt with in point 3. After deciding on the data, it was a question
of which tools we would work with. Although all participants had
either no or little experience with R, we decided to use this
programming language for the project in view of the content of the
semester and not ScraPy, which is written in Python and specially
designed for web scraping. So, it was a challenge to tackle and manage
the project with R. Due to the computationally intensive scripts and
the large amounts of data, we reached our limits with our computing
capacities. Some scripts ran for up to 3 days and required large
capacities of virtual memory. As a result, the work process slowed
down considerably and changes to the configurations for the LDA topic
model had to be planned and decided on what the benefits would
be. This meant a limitation in flexibility and experimentation in
designing the models. In order to have more resources, we asked the
computer science department if they could provide a suitable
machine. This was not the case and the university computer centre did
not have enough free space for us to work on their R cluster. One
possibility would have been to implement our modelling as a SLURM
job. This would have been exciting, but since we had no experience
with SLURM, the preparation would have been too time-consuming and
probably beyond the scope of our project. So, we had to deal with this
technical limitation and adapt the objective accordingly.


# Data Acquisition

# Analysis

The narrowing of the topic raises a few problems that should be addressed here. It must be clarified what is meant by "relevant”. As the research question is deliberately open-ended and invites further questions, it seems impossible to give a universal definition of the term "relevant". Relevance means a situational importance that someone attaches to something in a particular context. In the same way, relevance is always related to the everyday knowledge of the recipient and the importance of individual topics varies accordingly. In the analysis, therefore, one's own bias must always be taken into account as to which topics now seem relevant and important to the individual. When a user searches for topics related to transnational relations, he or she probably has certain expectations regarding the results. Based on these expectations, they can decide whether a topic is relevant to them or not. Therefore, the goal should be to make the analysis as neutral as possible. And accordingly, the tool, the topic model, should be as free of bias as possible.

## Methodology
According to the original project idea, we needed to gather resources
from various archives, which are accessible on the websites of each
country's parliament. As mentioned above, we gathered resources from
ten EU member countries. Although every country that we worked with
offered their minutes in a digital format, each of them was unique and
downloading them had us face different challenges.

After inspecting the archives, we could split them into two
groups. One group offers their protocols (at least for the last couple
of legislation periods) as PDF documents, the other as HTML
documents. Categorising them into one or the other group helped
deciding how to approach downloading the data. The PDF documents were
usually longer than their HTML counterparts, consisting of everything
that happened in one session, which in most cases corresponds to one
day. The first country for which we downloaded PDF documents was
Hungary, mainly for two reasons, the first being familiarity with the
language and the second was the convenience of downloading. As none of
the archives has a button to download everything, we had to write
scraper scripts which have done the task, which otherwise done
manually would have been very time consuming and error-prone. The
convenience Hungary provided was that all documents for a given
legislature period were gathered on a single HTML page, from which the
URLs could be easily extracted for downloading. One thing we wish is
that they had offered protocols from more legislature periods.

Other countries, such as Austria, did not have a single-page interface
from where we could have downloaded the data we needed. Austria had a
page for each session, which then included a link to the
download. Luckily, we did not have to write a scraper that opens each
of the hundreds of pages, since we discovered that they had a more
convenient and consistent URL naming scheme than Hungary has, which
enabled us to programatically generate the download URLs for each
session and period as well. The only thing we had to make sure is to
know how many sessions were held in each legislature period and how
the naming scheme worked, where the latter we had to do repeatedly
through some reverse engineering for each country. Similar to Austria
in this regard were Belgium, The Czech Republic, Spain and Italy.

Although there are more documents available for download from Austria,
documents that were created earlier than what we decided to work with
were not created using the PDF format, they were mostly scanned documents with
inconsistent formatting. Including these documents would have been
possible by implementing optical character recognition, but the size and
time-frame of the project did not allow us to explore every possible
document. It is also worth to mention, that in order to better
understand transnational relations, not only parliamentary debates are
useful but other institutions' publicly available documents as well,
but we did not include any of those in this project.

The Czech Republic also had a larger time-frame of documents
available, but the format would have been HTML documents for earlier
legislature periods, which we decided not to include. Spain had
periods in the archive, where they had a link to the given period, but
it was an empty directory. Ireland's stenographic protocols were
uploaded with a consistent naming scheme, which in itself would not
have been a problem, if the names have not had been consisted of the
date of the session and some constant value. In practice it meant a
computationally rather expensive pattern matching with regular
expressions in order to extract the URLs of the PDF files. Although
this operation consumed more computing resources than generating the
URLs according to a naming scheme, the overall bottleneck at this
stage was the number of documents to download, bandwidth at the
server's side and the fact that we could not implement a solution that
can handle multiple asynchronous downloads.

The next part was extracting raw text from the documents. For this
task, we used the pdftools[@libpdftools] package for R[@Rcore]. The
`pdf_data` function that
ships with it is able to extract text from PDF documents word by word,
while retaining metadata such as x and y coordinates for each word and
whether each word was at the end of a line. After some initial trial
and error with figuring out each country's page layout, we were able
to cut off page headers and footers, and pages which did not contain
either one of the two. This was very practical, since title pages and
tables of contents usually did not have headers (and footers).
Removing any miscellaneous text followed, this was done by
searching for the beginning and the end of each session, in each
language this corresponded to the following forms: "The session
started at ..." and "The session ended at ...". Only pages between
these two terms did we retain. Contrary to DOC, HTML and similar
document formats, text extracted from PDF documents remains
hyphenated. This meant that unhyphenating the text was essential to
prepare the raw data to train LDA models on it. This step was done
by concatenating every word at the end of a line with the first word
fragment of the following line, if it ended with a hyphen. This
procedure however, resulted in lines too long. Line breaks and any
unnecessary whitespace was removed next, as we did not need the
information of what each token's line number is. In order to obtain
cleaner results and to be on a par with the shorter HTML documents,
each session was split into smaller chunks of maximum 1000
tokens. Before this final step, many of them would have been more than
20,000 tokens long.

Extracting raw text from the PDF documents was generally a
straightforward process except for Belgium. The most notable
difference was that being a bilingual country, they offer their
minutes in a bilingual format, where each language is either on the
left or right side of the page. This would not have been a problem, if
the languages were always on the same side. This became obvious only
as we started to expand our initially rather small amount of data to
include more legislature periods from each country. This was done due
to concerns about lemma stemming, which we decided not to apply to our
data at all. This however, needed us increasing the amount of data at
hand. At the beginning only one period's documents were downloaded
from Belgium, where the sides for each language were consistent, but
this consistency changed as we introduced data from earlier
periods. We thought of multiple implementations to tackle this
problem, for example we had an observation, that even numbered
legislature periods tended to have French text on the right side, but
this was not always true, and it was unreliable to the point, that
using this method, many of the topics in the LDA topic model were in
fact not in French, but Dutch. In short, we were not satisfied with
this approach. Later on, we settled on an implementation, where the
program checked each document (e.g. 3rd period, Session no. 30., left
side) whether it was written in Dutch or French. Although this
approach was computationally more expensive than sheer guessing,
because it checked the probability of the possible languages for each
document by searching for stop words, this method turned out to work
very accurately. We still ended up not being able to use the first
period's documents, since the languages were not separated to either
side, but intermingled as a sentence-by-sentence translation, which in
some cases became paragraph-by-paragraph translation, or in extreme
cases some text was not translated at all. For the analysis, we used
only the French part of the text, since in theory it has the same
content as the Dutch part.

After extracting the text from the PDF documents, we collected the
resulting text files into one JSON file for each country, which were
then used to created the document term matrices.

The challenge with HTML countries was that they did not always have an
easily understandable URL naming scheme and that the sessions were
not on one page but scattered through multiple HTML documents. For
example debates from Poland were scattered through the websites of
individual politicians, which had to be then concatenated to get full
debates.

The following table summarizes the amount of data we were able to
gather for this project.


| Country            | No. of legislature periods | Time Range | Word Count           |
|--------------------|----------------------------|------------|----------------------|
| Austria            |                          6 | 1999--2019 |  51,722,689          |
| Belgium            |                          5 | 1999--2019 |  22,356,325 (French) |
| The Czech Republic |                          3 | 2010--2021 |  29,522,033          |
| France             |                          1 | 2012--2021 |  32,155,168          |
| Hungary            |                          2 | 2014--2021 |  25,297,392          |
| Ireland            |                          1 | 2016--2020 |  30,169,798          |
| Italy              |                          1 | 2013--2018 |  43,483,753          |
| The Netherlands    |                          3 | 2014--2021 |  11,916,017          |
| Poland             |                          9 | 1991--2021 | 116,433,805          |
| Spain              |                          5 | 2011--2021 |  19,491,066          |

# Analysis

## Method

Our initial approach to the analysis was to first create document term
matrices from the cleaned up data, then train an LDA topic model on
it. We first created document term matrices, where stop words
shipped with the SnowballC[@libSnowballC] package were removed, and
lemmas were
normalized using hunspell. After training the model on these matrices,
we began to notice multiple problems with this approach. The first
problem was that nearly all topics had similar top terms. These were
not stop words in the traditional sense, but nevertheless
irrelevant. This included words like 'congressman', 'house', 'please'
and 'thank you'. To remove these words, we first created an extended
list of words that we considered to be stop words. Following
additional research, we concluded, that it's not a good idea to remove
these words before training the model, as this would introduce
unwanted bias to the results and may even distract the algorithm to
find the topics. So we decided not to remove additional stop words
from the data.

During our research, we learnt that lemma stemming can also introduce
anomalies into the results. As Thomas Köntges pointed out in his
article[@UBHD-68613810],
increasing the data volume helps to maintain accuracy
when removing the lemma stemming step from the analysis. We needed to
increase the data volume, since most of the languages we worked with
have a higher morphological complexity than English. We had collected
only one legislation period's data from each PDF country. The amount
of data for most countries was below 13 million tokens. At this point
in the analysis we decided to include more legislation periods, so we
went back to the data acquisition part to get more data. Also at this
time, we decided to cut long documents into shorter parts, which had
us modify the scraper scripts accordingly. The difficulty with
Belgium's parliamentary minutes mentioned in point 3 became obvious
during this step.

As for the topic modelling algorithm, we started with 20 topics. Since
many of the topics were not really topics but rather a collection of
common words in a parliament, we increased the number of topics. These
changes led to our problem with computing resources. Running the
algorithm with the increased data set and increased topic number also
increased computing time and memory usage. Since the algorithm is not
parallelised, we could run several country's scripts at the same time,
because the memory usage also allowed this. After the increase, this
was not possible any more, even on multiple CPU systems, because
increased memory usage usually did not allow to do so. This slowed
down our progress significantly. For countries that had less data,
running the algorithm took between 8 to 10 hours, for more data,
it could take 2--3 days. Changing one parameter cost us from several days
up to one week of computing time, in which we could not further improve
our approach to analysis. Finally, as we increased the number of
topics to 100, we stopped changing the parameters given to the
algorithm and started to work more on the analysis.

Our first approach was to (manually) search for country names, city
names or topics that had to do with other countries. All countries had
references to Belgium, which we explained with the European Committee
having its headquarters in Brussels, Belgium. There were also mentions
of the Euro crisis, refugee crisis, and in newer documents the
coronavirus pandemic. While Austria had a topic that most probably had
to do with nuclear energy plants, which included references to the
Czech Republic, Hungary also had a similar topic with references to
Russia. The reason for this is that in the last two legislation
periods the Hungarian government decided to extend Hungary's only one
nuclear plant funded with Russian credit. Hungary also had a topic,
which, although did not contain many references to countries, it
included the name of George Soros. With some background knowledge,
this topic can also be counted as a transnational relation. The
reasoning behind this has to do with the Orban regime's blaming the
EU's proposition of refugee quotas on the influence of Soros and his
Open Society Foundations, even accusing him of funding human
trafficking across the Mediterranean.

This manual searching however, is not reproducible, and it is easy to
introduce our own biases coming from our knowledge of politics or lack
thereof into the results. In order to avoid these negative effects, we
opted for another approach.

(Here comes Isidor's latest improvements for the analysis, including
results and visualizaion.)

## Challenges

- enormous computing resource (time, memory)
- unfamiliarity with the theoretical background of LDA topic modeling, lack of experience
- languages that none of us speaks

## Results

- excerpts from the results as a table representation of the data frames
- statistics
- word clouds

# Conclusion

# Remarks

# References

e.g. websites of various parliaments, research papers we read on the topic, libraries we used for the analysis etc.
