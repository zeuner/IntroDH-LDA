---
output:
  pdf_document:
    highlight: tango
    number_sections: true
documentclass: scrreprt
fontsize: 12pt
---
```{=latex}
\begin{titlepage}
\centering
\vfill{}
\input{uni_logo.pdf_tex} \par
\vspace{3cm}
{\bfseries\huge Analyzing Transnational Relationships in Parliament Debates \par}
\vspace{1cm}
{\large Agoston Volcz, Jonathan Weinmann, Isidor Zeuner \par
March 14. 2021}

\vfill
Fakultät für Mathematik und Informatik \\
Seminar: 10-207-0001 Einführung in die Digital Humanities \\
Lecturer: Thomas Köntges, Ph.D. \\
Semester: WS 2020/2021


\end{titlepage}
\setcounter{page}{1}
\tableofcontents
```

Abstract

# Introduction

Parliaments usually make corrected versions of their plenary minutes available for public inspection on the internet. The minutes contain the speeches of the MPs, reports and debates. These documents are accessible to everybody, but the large amount of the documents makes it difficult to index and search. And it is even more difficult to recognise complex topics, connections and "between the lines" content. In order to be able to make a meaningful analysis of these provided documents, preliminary work must be done. The documents must be downloaded, cleaned up in a way that elements not carrying any information will be removed. Furthermore, they have to be converted into a structured format so that the machine can work with them. The computer must then be trained to "understand" the documents and produce human readable output. This preliminary work is to be done with this project.
Analysing parliament debates on European level was the basic idea of our Project. Away from the topic, it was the interest in text-based analysis and topic modelling that brought us together as a group. In the beginning we didn’t have a concrete idea of a specific research question. We thought it would be useful to have a simpler and clearer way of giving an insight into the debates of the parliaments, as it is hardly possible to keep track of the unbelievably large amount of the minutes of the meetings. And it is even more difficult to deal specifically with a particular topic within them. Based on this impulse, the idea came up to look for transnational relations that emerge in the debates and using Natural Language Processing and LDA Topic Modelling as a tool. During the implementation of the project, it became increasingly clear that the focus was shifting. It was no longer just about answering this one question that had been asked with the technical tools, but rather to develop a tool with which it is possible to work on our initial research idea and similar questions based on it. As well limits of the method became visible and that it can only cover a part. So, we came up to rephrase the question like this: To what extent can our technical approach help finding transnational relations between countries from the text corpora? 
During the report, it should be made clear how the tool was developed, at which points we reached our limits and which solutions we considered. The first part of the report describes the organisational process of the project. How did we communicate and share our results? Decisions which data and which tools we will use will be presented in this part. The focus will then be on data collection, conversion and the topic model. It is about building a tool for web-scraping and adapting it to the different formats offered on the government websites. The final step was to create the topic model and to analyse it. The emphasis in our project is more on the technical aspect and the handling, controlling and processing of the data sets as well as the formation of the topic model than on the analysis. 

# Organisation 

The first thing was to set up an infrastructure in which we could communicate. For this we first used ChaosPad. This made it possible for everyone to exchange initial ideas. We decided to use the pad for the more humanities questions and topics and to switch to Git and Github for the exchange and organisation of code. After setting up a RocketChat-Channel for organisational purposes, however, most communication, both organisational, technical and intellectual, quickly took place there. 							
Since the topic of the project is the analysis of parliamentary debates of different but of course as many European countries as possible we already encountered the first difficulty and above all the first limitation which would mean a bias in the analysis. In order to interpret the debates as accurately as possible, it is a prerequisite that we speak the languages or at least have a certain understanding of them. So, we ended up working with data from the parliaments of Austria, Belgium, the Czech Republic, Hungary, Italy, Ireland, Poland, France and the Netherlands. It is important to emphasise that we are limited at this point and that this is not an ideal condition. The parliamentary minutes were made available on the websites of the respective governments. However, it turned out that both the scope in terms of years and the type of document as well as the presentation in the documents varied considerably. The detailed data collection is dealt with in point 3. After deciding on the data, it was a question of which tools we would work with. Although all participants had either no or little experience with R, we decided to use this programming language for the project in view of the content of the semester and not ScraPy, which is written in Python and specially designed for web scraping. So, it was a challenge to tackle and manage the project with R. Due to the computationally intensive scripts and the large amounts of data, we reached our limits with our computing capacities. Some scripts ran for up to 3 days and required large capacities of virtual memory. As a result, the work process slowed down considerably and changes to the configurations for the LDA topic model had to be planned and decided on what the benefits would be. This meant a limitation in flexibility and experimentation in designing the models. In order to have more resources, we asked the computer science department if they could provide a suitable machine. This was not the case and the university computer centre did not have enough free space for us to work on their R cluster. One possibility would have been to implement our modelling as a SLURM job. This would have been exciting, but since we had no experience with SLURM, the preparation would have been too time-consuming and probably beyond the scope of our project. So, we had to deal with this technical limitation and adapt the objective accordingly. 


# Data Acquisition
- EU parliaments
- languages that we speak, also languages which we don't

## Deciding on sources

- Austria:
  Legislation periods:
  - XXI: 29.10.1999--19.12.2002
  - XXII: 20.12.2002--29.10.2006
  - XXIII: 30.10.2006--27.10.2008
  - XXIV: 28.10.2008--28.10.2013
  - XXV: 29.10.2013--08.11.2017
  - XXVI: 09.11.2017--22.10.2019

  862 PDF documents / sessions, 51,722,689 total word count after removing table of contents etc.
  
- Belgium:
  Legislation periods:
  - 1995--1999
  - 1999--2003
  - 2003--2007
  - 2007--2010
  - 2010--2014
  - 2014--2019

  1,097 PDF documents / sessions, 43,741,136 total word count after removing table of contents etc.
  
- Czech Republic:
  Legislation periods:
  - 2010--2013
  - 2013--2017
  - 2017--2021*

  186 PDF documents / sessions, 29,522,033 total word count after removing table of contents etc.
  
- France
- Hungary:
  Legislation periods:
  - 2014--2018
  - 2018--2021*
  
  451 PDF documents / sessions, 25,297,392 total word count after removing table of contents etc.
  
- Ireland:
  Dáil debates from the 32nd Dáil, 2016--2020 (last session in December 2019)

  394 PDF documents / sessions, 30,169,798 total word count after removing table of contents etc.
  
- Italy:
  Legislature period:
  - XVII: 15.03.2013--22.03.2018

  923 PDF documents / sessions, 43,483,753 total word count after removing table of contents etc.
- The Netherlands
- Poland
- Spain:
  Legislature periods:
  - X: 13.12.2011--27.10.2015
  - XI: 13.01.2016--03.05.2016
  - XII: 19.07.2016--05.03.2019
  - XIII: 21.05.2019--24.09.2019
  - XIV: 03.12.2019--2021*

  331 PDF documents / sessions, 19,491,066 total word count after removing table of contents etc.

## Downloading the data

- we wrote scraper scripts
- There were two kinds of archives: PDF and HTML. Some of the archives which offer PDF documents to download didn't have a consistent URL naming scheme for the documents, which had us extract the urls from their websites.
- How did we scrape data which were in HTML documents?

### Converting the documents to plain text

- How did we convert HTML documents to plain text?
- PDF documents were converted in each scraper script. We had to analyze the page layout of each country's protocols. Following steps were necessary to produce usable data: removing page headers and footers, removing empty pages, removing title page, table of contents etc., unhypthenating the text, splitting the documents to smaller chunks of maximum 1000 words size (here a reference to the paper which explained why it's important to do so), systematic document naming which contains information about country, legislation period, session number in a period.

### Challenges

- inconsistent url naming
- inconsistent page layout also inside one country's documents. For example Belgium supplies their protocols in a bilingual form, in which every page has two columns, each written either in French or Dutch. Whether the left or right column is French was inconsistent. Even numbered periods generally had French on the right side of the page, but this wasn't always true, occasionally they swapped the order of the columns.
- bandwidth

## What data did we end up using?

- We could not use the documents from Belgium's 1st period, because although laid out in two columns, the languages weren't written on either side, but interlaced sentence by sentence.

# Analysis

The narrowing of the topic raises a few problems that should be addressed here. It must be clarified what is meant by "relevant”. As the research question is deliberately open-ended and invites further questions, it seems impossible to give a universal definition of the term "relevant". Relevance means a situational importance that someone attaches to something in a particular context. In the same way, relevance is always related to the everyday knowledge of the recipient and the importance of individual topics varies accordingly. In the analysis, therefore, one's own bias must always be taken into account as to which topics now seem relevant and important to the individual. When a user searches for topics related to transnational relations, he or she probably has certain expectations regarding the results. Based on these expectations, they can decide whether a topic is relevant to them or not. Therefore, the goal should be to make the analysis as neutral as possible. And accordingly, the tool, the topic model, should be as free of bias as possible.

## Methodology

- Creating document term matrices (without lemma stemming)
- LDA models (number of topics, iterations other parameters)


## Challenges

- enormous computing resource (time, memory)
- unfamiliarity with the theoretical background of LDA topic modeling, lack of experience
- languages that none of us speaks

## Results

- excerpts from the results as a table representation of the data frames
- statistics
- word clouds

# Conclusion

# Remarks

# References

e.g. websites of various parliaments, research papers we read on the topic, libraries we used for the analysis etc.
