---
output:
    bookdown::pdf_document2:
    highlight: tango
    citation_package: biblatex
    number_sections: true
documentclass: scrartcl
bibliography: report.bib
biblio-style: authoryear
header-includes:
  - \usepackage{setspace}
  - \onehalfspacing
fontsize: 12pt
---
```{=latex}
\begin{titlepage}
\centering
\vfill{}
\input{uni_logo.pdf_tex} \par
\vspace{3cm}
{\bfseries\huge Analysing Transnational Relationships in Parliament Debates \par}
\vspace{1cm}
{\large Agoston Volcz, Jonathan Weinmann, Isidor Zeuner \par
March 14. 2021}

\vfill
Fakultät für Mathematik und Informatik \\
Seminar: 10-207-0001 Einführung in die Digital Humanities \\
Lecturer: Thomas Köntges, Ph.D. \\
Semester: WS 2020/2021


\end{titlepage}
\setcounter{page}{1}
\tableofcontents
\clearpage
```
# Introduction

Parliaments usually make corrected versions of their plenary minutes
available for public inspection on the internet. The minutes contain
the speeches of the MPs, reports and debates. These documents are
accessible to everybody, but the large amount of the documents makes
it difficult to index and search. And it is even more difficult to
recognise complex topics, connections and "between the lines"
content. In order to be able to make a meaningful analysis of these
provided documents, preliminary work must be done. The documents must
be downloaded, cleaned up in a way that elements not carrying any
information will be removed. Furthermore, they have to be converted
into a structured format so that the machine can work with them. The
computer must then be trained to "understand" the documents and
produce human readable output. This preliminary work is to be done
with this project.

Analysing parliament debates on European level was the basic idea of
our Project. Away from the topic, it was the interest in text-based
analysis and topic modelling that brought us together as a group. In
the beginning we didn’t have a concrete idea of a specific research
question. We thought it would be useful to have a simpler and clearer
way of giving an insight into the debates of the parliaments, as it is
hardly possible to keep track of the unbelievably large amount of the
minutes of the meetings. And it is even more difficult to deal
specifically with a particular topic within them. Based on this
impulse, the idea came up to look for transnational relations that
emerge in the debates and using Natural Language Processing and LDA
Topic Modelling as a tool. During the implementation of the project,
it became increasingly clear that the focus was shifting. It was no
longer just about answering this one question that had been asked with
the technical tools, but rather to develop a tool with which it is
possible to work on our initial research idea and similar questions
based on it. As well limits of the method became visible and that it
can only cover a part. So, we came up to rephrase the question like
this: To what extent can our technical approach help finding
transnational relations between countries from the text corpora?

During the report, it should be made clear how the tool was developed,
at which points we reached our limits and which solutions we
considered. The first part of the report describes the organisational
process of the project. How did we communicate and share our results?
Decisions which data and which tools we will use will be presented in
this part. The focus will then be on data collection, conversion and
the topic model. It is about building a tool for web-scraping and
adapting it to the different formats offered on the government
websites. The final step was to create the topic model and to analyse
it. The emphasis in our project is more on the technical aspect and
the handling, controlling and processing of the data sets as well as
the formation of the topic model than on the analysis.

# Organisation

The first thing was to set up an infrastructure in which we could
communicate. For this we first used ChaosPad. This made it possible
for everyone to exchange initial ideas. We decided to use the pad for
the more humanities questions and topics and to switch to Git and
Github for the exchange and organisation of code. After setting up a
RocketChat-Channel for organisational purposes, however, most
communication, both organisational, technical and intellectual,
quickly took place there.

Since the topic of the project is the analysis of parliamentary
debates of different but of course as many European countries as
possible we already encountered the first difficulty and above all the
first limitation which would mean a bias in the analysis. In order to
interpret the debates as accurately as possible, it is a prerequisite
that we speak the languages or at least have a certain understanding
of them. So, we ended up working with data from the parliaments of
Austria, Belgium, the Czech Republic, Hungary, Italy, Ireland, Poland,
France and the Netherlands. It is important to emphasise that we are
limited at this point and that this is not an ideal condition. The
parliamentary minutes were made available on the websites of the
respective governments. However, it turned out that both the scope in
terms of years and the type of document as well as the presentation in
the documents varied considerably. The detailed data collection is
dealt with in point 3. After deciding on the data, it was a question
of which tools we would work with. Although all participants had
either no or little experience with R, we decided to use this
programming language for the project in view of the content of the
semester and not ScraPy, which is written in Python and specially
designed for web scraping. So, it was a challenge to tackle and manage
the project with R. Due to the computationally intensive scripts and
the large amounts of data, we reached our limits with our computing
capacities. Some scripts ran for up to 3 days and required large
capacities of virtual memory. As a result, the work process slowed
down considerably and changes to the configurations for the LDA topic
model had to be planned and decided on what the benefits would
be. This meant a limitation in flexibility and experimentation in
designing the models. In order to have more resources, we asked the
computer science department if they could provide a suitable
machine. This was not the case and the university computer centre did
not have enough free space for us to work on their R-Studio servers. One
possibility would have been to implement our modelling as a SLURM
job to run on the URZ big data cluster. This would have been exciting,
but since we had no experience with SLURM, we had to decide not to depend
on this option. So, we had to deal with this
technical limitation and adapt the objective accordingly.


# Data Acquisition

According to the original project idea, we needed to gather resources
from various archives, which are accessible on the websites of each
country's parliament. As mentioned above, we gathered resources from
ten EU member countries. Although every country that we worked with
offered their minutes in a digital format, each of them was unique and
downloading them had us face different challenges.

After inspecting the archives, we could split them into two
groups. One group offers their protocols (at least for the last couple
of legislation periods) as PDF documents, the other as HTML
documents. Categorising them into one or the other group helped
deciding how to approach downloading the data. The PDF documents were
usually longer than their HTML counterparts, consisting of everything
that happened in one session, which in most cases corresponds to one
day.

Some countries, for example the Netherlands, offered both HTML-based
and PDF-based protocols, and not necessarily for the same data. In
order to be able to work on all the selected countries while still
taking into account the limited time available for the project, for
each country only one type of protocols was selected.

## PDF-based data {#pdf-scraping}

The first country for which we downloaded PDF documents was
Hungary, mainly for two reasons, the first being familiarity with the
language and the second was the convenience of downloading. As none of
the archives has a button to download everything, we had to write
scraper scripts which have done the task, which otherwise done
manually would have been very time consuming and error-prone. The
convenience Hungary provided was that all documents for a given
legislature period were gathered on a single HTML page, from which the
URLs could be easily extracted for downloading. One thing we wish is
that they had offered protocols from more legislature periods.

Other countries, such as Austria, did not have a single-page interface
from where we could have downloaded the data we needed. Austria had a
page for each session, which then included a link to the
download. Luckily, we did not have to write a scraper that opens each
of the hundreds of pages, since we discovered that they had a more
convenient and consistent URL naming scheme than Hungary has, which
enabled us to programatically generate the download URLs for each
session and period as well. The only thing we had to make sure is to
know how many sessions were held in each legislature period and how
the naming scheme worked, where the latter we had to do repeatedly
through some reverse engineering for each country. Similar to Austria
in this regard were Belgium, The Czech Republic, Spain and Italy.

Although there are more documents available for download from Austria,
documents that were created earlier than what we decided to work with
were not initially created as PDF files, they were mostly scanned documents
with inconsistent formatting. Including these documents would have been
possible by implementing optical character recognition, but the size and
time-frame of the project did not allow us to explore every possible
document. It is also worth to mention, that in order to better
understand transnational relations, not only parliamentary debates are
useful but other institutions' publicly available documents as well,
but we did not include any of those in this project.

The Czech Republic also had a larger time-frame of documents
available, but the format would have been HTML documents for earlier
legislature periods, which we decided not to include. Spain had
periods in the archive, where they had a link to the given period, but
it was an empty directory. Ireland's stenographic protocols were
uploaded with a consistent naming scheme, which in itself would not
have been a problem, if the names have not had been consisted of the
date of the session and some constant value. In practice it meant a
computationally rather expensive pattern matching with regular
expressions in order to extract the URLs of the PDF files. Although
this operation consumed more computing resources than generating the
URLs according to a naming scheme, the overall bottleneck at this
stage was the number of documents to download, bandwidth at the
server's side and the fact that we could not implement a solution that
can handle multiple asynchronous downloads.

The next part was extracting raw text from the documents. For this
task, we used the `pdftools`[@libpdftools] package for R[@Rcore]. The
`pdf_data` function that
ships with it is able to extract text from PDF documents word by word,
while retaining metadata such as x and y coordinates for each word and
whether each word was at the end of a line. After some initial trial
and error with figuring out each country's page layout, we were able
to cut off page headers and footers, and pages which did not contain
either one of the two. This was very practical, since title pages and
tables of contents usually did not have headers (and footers).
Removing any miscellaneous text followed, this was done by
searching for the beginning and the end of each session, in each
language this corresponded to the following forms: "The session
started at ..." and "The session ended at ...". Only pages between
these two terms did we retain. Contrary to DOC, HTML and similar
document formats, text extracted from PDF documents remains
hyphenated. This meant that unhyphenating the text was essential to
prepare the raw data to train LDA models on it. This step was done
by concatenating every word at the end of a line with the first word
fragment of the following line, if it ended with a hyphen. This
procedure however, resulted in lines too long. Line breaks and any
unnecessary whitespace was removed next, as we did not need the
information of what each token's line number is. In order to obtain
cleaner results and to be on a par with the shorter HTML documents,
each session was split into smaller chunks of maximum 1000
tokens. Before this final step, many of them would have been more than
20,000 tokens long.

Extracting raw text from the PDF documents was generally a
straightforward process except for Belgium. The most notable
difference was that being a bilingual country, they offer their
minutes in a bilingual format, where each language is either on the
left or right side of the page. This would not have been a problem, if
the languages were always on the same side. This became obvious only
as we started to expand our initially rather small amount of data to
include more legislature periods from each country. This was done due
to concerns about lemma stemming, which we decided not to apply to our
data at all. This however, needed us increasing the amount of data at
hand. At the beginning only one period's documents were downloaded
from Belgium, where the sides for each language were consistent, but
this consistency changed as we introduced data from earlier
periods. We thought of multiple implementations to tackle this
problem, for example we had an observation, that even numbered
legislature periods tended to have French text on the right side, but
this was not always true, and it was unreliable to the point, that
using this method, many of the topics in the LDA topic model were in
fact not in French, but Dutch. In short, we were not satisfied with
this approach. Later on, we settled on an implementation, where the
program checked each document (e.g. 3rd period, Session no. 30., left
side) whether it was written in Dutch or French. Although this
approach was computationally more expensive than sheer guessing,
because it checked the probability of the possible languages for each
document by tokenizing it using the `koRpus`[@libkoRpus] package and
searching for stop words, this method turned out to work
very accurately. We still ended up not being able to use the first
period's documents, since the languages were not separated to either
side, but intermingled as a sentence-by-sentence translation, which in
some cases became paragraph-by-paragraph translation, or in extreme
cases some text was not translated at all. For the analysis, we used
only the French part of the text, since in theory it has the same
content as the Dutch part.

After extracting the text from the PDF documents, we collected the
resulting text files into one JSON file for each country using
the `jsonlite`[@libjsonlite] package, which were
then used to created the document term matrices.

## HTML-based data

For HTML-based protocols, the raw data contained more semantic structure
in comparison with the PDF format that focuses mostly on how the content
is displayed. This led both to opportunities and challenges.

On the one hand, it was often easier to distinguish given debate items
from the complete parliament meeting protocols. For the Netherlands and
France, the protocols could be split based on specific HTML elements or
attributes that always occurred at the beginning of a debate item,
and only there. For Poland, there was even a single web page for every
speech, so individual debate items had to be concatenated from multiple
pages after identifying the debate item for each of the pages.

On the other hand, focusing on the semantic structure without the
parliaments using a common standard for this, much of the logic had
to be written individually for each country. In case of Poland,
there were even two different archive servers that applied a different
structure to the data, and had to be tackled individually. Furthermore,
the representation of the debates as multiple speech pages for Poland
led to a huge amount of individual web pages to be downloaded. Since
every individual web page to download meant another transaction that
could eventually fail, basic caching of already fetched web pages was
implemented so subsequent runs would not need to fetch them again.
Using the `digest`[@libdigest] library, every URL was assigned a
cache filename supposed to be unique under reasonable assumptions.

After extracting the structure from the raw HTML data and assigning
the HTML lines to debate items, getting the actual text was usually
straightforward because it could be achieved by stripping off the
HTML tags.

As with the PDF based protocols, the resulting data was exported
to JSON files in order to have a common data format for further
processing.

## Results

The following table summarizes the amount of data we were able to
gather for this project.


| Country            | No. of legislature periods | Time Range | Word Count           |
|--------------------|----------------------------|------------|----------------------|
| Austria            |                          6 | 1999--2019 |  51,722,689          |
| Belgium            |                          5 | 1999--2019 |  22,356,325 (French) |
| The Czech Republic |                          3 | 2010--2021 |  29,522,033          |
| France             |                          1 | 2012--2021 |  32,155,168          |
| Hungary            |                          2 | 2014--2021 |  25,297,392          |
| Ireland            |                          1 | 2016--2020 |  30,169,798          |
| Italy              |                          1 | 2013--2018 |  43,483,753          |
| The Netherlands    |                          3 | 2014--2021 |  11,916,017          |
| Poland             |                          9 | 1991--2021 | 116,433,805          |
| Spain              |                          5 | 2011--2021 |  19,491,066          |

# Analysis

## Goal

The narrowing of the topic raises a few problems that should be addressed here. It must be clarified what is meant by "relevant”. As the research question is deliberately open-ended and invites further questions, it seems impossible to give a universal definition of the term "relevant". Relevance means a situational importance that someone attaches to something in a particular context. In the same way, relevance is always related to the everyday knowledge of the recipient and the importance of individual topics varies accordingly. In the analysis, therefore, one's own bias must always be taken into account as to which topics now seem relevant and important to the individual. When a user searches for topics related to transnational relations, he or she probably has certain expectations regarding the results. Based on these expectations, they can decide whether a topic is relevant to them or not. Therefore, the goal should be to make the analysis as neutral as possible. And accordingly, the tool, the topic model, should be as free of bias as possible.

## Method

### Topic modeling

In contrast to the highly diverse situation in the data acquisition step,
in the analysis part the idea was that the different text corpora should
be handled as uniformly as possible, with handling differences being mostly
limited to the different languages. So, the functions were implemented
to work on all the countries, with all remaining differences being stored
in a hash table created using the `hash`[@libhash] package. It turned out
that it was sufficient to differentiate language and country codes and
the source for the stopword list since some languages were not
represented in the `SnowballC`[@libSnowballC] package, while others
were missing in the lists from the `stopwords`[@libstopwords] package.

Our initial approach to the analysis was to first create document term
matrices from the cleaned up data, then train an LDA topic model on
it. We first created document term matrices, where stop words
shipped with the `SnowballC`[@libSnowballC] package were removed, and
lemmas were
normalized using `hunspell`[@libhunspell]. After training the model
on these matrices,
we began to notice multiple problems with this approach. The first
problem was that nearly all topics had similar top terms. These were
not stop words in the traditional sense, but nevertheless
irrelevant. This included words like 'congressman', 'house', 'please'
and 'thank you'. To remove these words, we first created an extended
list of words that we considered to be stop words. Following
additional research, we concluded, that it's not a good idea to remove
these words before training the model, as this would introduce
unwanted bias to the results and may even distract the algorithm to
find the topics. So we decided not to remove additional stop words
from the data.

During our research, we learnt that lemma stemming can also introduce
anomalies into the results. As Thomas Köntges pointed out in his
article[@UBHD-68613810],
increasing the data volume helps to maintain accuracy
when removing the lemma stemming step from the analysis. We needed to
increase the data volume, since most of the languages we worked with
have a higher morphological complexity than English. We had collected
only one legislation period's data from each PDF country. The amount
of data for most countries was below 13 million tokens. At this point
in the analysis we decided to include more legislation periods, so we
went back to the data acquisition part to get more data. Also at this
time, we decided to cut long documents into shorter parts, which had
us modify the scraper scripts accordingly. The difficulty with
Belgium's parliamentary protocols mentioned in section \@ref(pdf-scraping)
became obvious during this step.

As for the topic modelling algorithm, we started with 20 topics. Since
many of the topics were not really topics but rather a collection of
common words in a parliament, we increased the number of topics. This
led to challenges regarding computational costs as described in
section \@ref(computing-requirements). Consequently, as we increased
the number of topics to 100, we stopped changing the parameters
given to the algorithm and started to work more on the
analysis.

### Transnational relations {#country-references}

Our first approach was to (manually) search for country names, city
names or topics that had to do with other countries. All countries had
references to Belgium, which we explained with the European Committee
having its headquarters in Brussels, Belgium. There were also mentions
of the Euro crisis, refugee crisis, and in newer documents the
coronavirus pandemic. While Austria had a topic that most probably had
to do with nuclear energy plants, which included references to the
Czech Republic, Hungary also had a similar topic with references to
Russia. The reason for this is that in the last two legislation
periods the Hungarian government decided to extend Hungary's only one
nuclear plant funded with Russian credit. Hungary also had a topic,
which, although did not contain many references to countries, it
included the name of George Soros. With some background knowledge,
this topic can also be counted as a transnational relation. The
reasoning behind this has to do with the Orban regime's blaming the
EU's proposition of refugee quotas on the influence of Soros and his
Open Society Foundations, even accusing him of funding human
trafficking across the Mediterranean.

This manual searching however, is not reproducible, and it is easy to
introduce our own biases coming from our knowledge of politics or lack
thereof into the results. In order to avoid these negative effects, we
opted for another approach that also allows for pre-filtering nearly
arbitary amounts of text in preparation for human research to be
done subsequently.

In order to gather references to other countries in a fully automated manner,
they had to be found by matching the terms extracted from the documents.

Terms to search for could be extracted from the data collected by the
Wikipedia project. It has encyclopedia pages for every country in nearly
every language, and it links them together through interlanguage links,
which allowed for straightforward extraction of the multi-language
translations from the HTML pages, and of the country names directly
from the URLs using the `urltools`[@liburltools] package.

For performance reasons, matching the search terms against the document
terms was done using the document term matrices already computed for
the topic modeling. Since the search terms might occur as inflected
forms depending on the language, this step required morphological
normalization using the Snowball stemmer[@libSnowballC]. With a stemming
function $s$ and a country name consisting of $k$ words $w_{i}$,
we count the words $w$ in the document where $s(w) = s(w_{i})$. Then,
the word from the country name with the minimum matching word count
is considered the frequency of the country name in the document.

It should be
mentioned that this approach is of limited accuracy. It can create false
positives because the document term matrices lack information about the
positions of the terms in the documents. For example, an English-language
text might be related to the country "South Africa" if it
contains the terms "south" and "africa" which
are both expected to also occur in other contexts. At the same time,
the German-language translation "Südafrika" should be
fairly unambiguous. This must be kept in mind when interpreting the results
since it means that text corpora in different languages cannot be
meaningfully compared to each other with respect to extracted country
references. It is expected that this effect can be reduced by working
with n-grams in the document term matrices. With increasing $n$, this
is expected to lead to results comparable to a phrase search in a web
search engine.

At this point, there is the topic modeling result that can be transformed to
a matrix of document-topic frequencies, and a matrix of document-country
frequencies. These can be combined to a topic-country matrix using a
matrix cross-product.

### Reporting and visualization

For understanding what the different latent topics are about, the
`LDAvis`[@libLDAvis] package proved very useful since it comes with
a web-based interactive interface where different parameters can be
quickly tested for their effect. However, for reporting purposes, it
seemed preferable to calculate and plot the corresponding term relevance
measure[@sievert-shirley-2014-ldavis] directly in R. This way, the
plotted bar graphs would always be exactly reproducible from the data.

For visualizing the computed correlations between latent topics and
country references, working with a world map was envisioned, which
was available from the `maps`[@libmaps] package. However, this would
not map to the country reference data out of the box. On the one hand,
the country names retrieved as described in section
\@ref(country-references) mostly referred to sovereign states.
On the other hand, the names in the map data referred to regions,
which may or may not have outer sovereignty, or may even be subject
of disputes between different countries. For the purpose of this project
and under consideration of the limited timeframe, only a rather rough
mapping could be created, which maps external territories to the
corresponding countries without taking into account to what extent
there may be partial outer sovereignty, and where regions with very
complex or disputed territorial situation were left out. This does
also mean that an unintended bias could not be avoided at this stage.
But being careful is mandated anyway when interpreting the resulting
data, since the text matching of country names does also not guarantee
that the text actually referred to a sovereign country rather than
a region with a similar name. To prepare the visualizations which
were all required to work on the data referring to sovereign countries,
a spatial polygons data structure was created from the world map using the
`maptools`[@libmaptools] package and augmented with the region names
mapped to countries.

In order to get a visual overview over what latent topics show a clear
correlation to particular countries, plotting pie charts on a world map
seemed promising. An existing implementation of pie charts for geospatial
data was available through the `marmap`[@libmarmap] package. It turned
out that more than 200 countries relating to 100 topics, leading to
more than 20,000 data points, was just too much information to be visualized
on one map. In the end, a trade-off had to be implemented here where
only a subset of the topics were plotted into the pie charts. To choose
them, a matrix was computed which contained, for each country, the fractions
of the individual topics when compared to the sum of all topics found in
connection with that country. Here, a high fraction would mean that a
topic was much more frequent than the other topics with respect to the
particular country. So, the maximum over all countries, computed using
the `Rfast`[@libRfast] package, would be high for a topic if there
was at least one country where the topic exceeded the other topics
considerably. By that logic, the top ten topics with respect to the
maximum fraction were chosen to be visualized since they were
expected to show most selectiveness with respect to different countries.

For a given topic, its correlation to different countries was visualized
using a diffusion cartogram. To implement this, the `getcartr`[@libgetcartr]
package could be used which provides an interface to compute a
transformed map based on a mapping variable to substitute the land area
for each spatial polygon. The idea was to have countries show up bigger
on the map if they are related to the given topic more than on average,
and to show up smaller if it happens less than on average. This meant that
the mapping variable could not be proportional to the topic frequency
itself since this would be, on average, in the same dimension for all
countries, thereby regularly making all small countries show up bigger
and all big countries smaller. Instead, in order to achieve a map distortion
relative to the original country areas, the mapping variable had to be
multiplied by the factor of the original country area in the mercator
projection. This could be computed from the spatial polygons using the
`UScensus2000`[@libUScensus2000] package. This way, someone who is used to
how the world map looks like in the mercator projection would see where
the map differs. The downside is that countries which have a very small
land area are hardly noticeable on the cartogram no matter how focused
the topic is on them.

Finally, even for writing the project report itself, the project
infrastructure based on the R language proved useful. Using the
`rmarkdown`[@librmarkdown] package, the report including embedded
visualizations could be written mostly in markdown syntax and then
rendered to a PDF document. This way, the report document could easily
be updated based on new visualizations created from the data. Later
it turned out that using the `bookdown`[@libbookdown] package even
allows for automatically updated in-document references to other sections.

## Challenges

TODO: narrative formulation
- enormous computing resource (time, memory)
- unfamiliarity with the theoretical background of LDA topic modeling, lack of experience
- languages that none of us speaks

### Computing requirements {#computing-requirements}

In the process of continuously refining the analysis approach, it turned out
that considerable computing resources are useful for performing this kind
of analysis. For one text corpus, training an LDA model with 100 topics for
500 iterations required up to 2,7 GiB of virtual memory when the corpora
were preprocessed
using morphological normalization. Without morphological normalization,
up to 8,0 GiB of virtual memory were required. Furthermore, one text
corpus required between 8 and 87 hours depending on its size on a
student laptop, and could not be sped up by parallelization because
of the iterative nature of the LDA
algorithm implemented in the `textmineR`[@libtextmineR] package.
Due to the high memory
usage, possibilities for parallel processing of multiple text corpora
were limited on commodity hardware. Consequently, the text corpora
had to be processed mostly one after another, leading to a long running
time for getting the results from changed LDA parameters.

It is possible to cope with this challenge by making use of the university's
scientific computing infrastructure. However, since we became aware of
the high requirements late throughout
the limited project handling time, and since it takes time to negotiate
computing access and to implement the necessary configuration changes,
we could only make limited use of this possibility. This should be considered
as a valuable insight for future similar projects, where computing access
should be negotiated from the beginning.

One limitation even with the University's scientific computing resources
that is relevant for similar projects is that while they are much more scalable
than the hardware that could be used before, one hard limitation is the per-node
memory size which can still restrict what can be achieved with large text
corpora when including n-gram terms.

## Results

TODO: narrative formulation
- excerpts from the results as a table representation of the data frames
- statistics
- word clouds

# Conclusion

# Future work

## Data completeness

Future work could concentrate on trying to make the available data for each
country more comprehensive, for example by processing both HTML and PDF
protocols where they are available. This would require strategies for
deduplicating the data. It would enable more detailed analyses of specific
time intervals when it can be ensured that there is data from all the
countries for the chosen time interval.

## Scalability

The size of the text corpora, while enabling training of the LDA models without
the need for morphological normalization, also limited what could be achieved
using the available computing resources. In particular, due to the sequential
nature of the LDA training algorithm, parallelization was of limited use.
It could be researched whether algorithms adapted for parallelization,
like Sparse Partially Collapsed MCMC[@magnusson2017sparse], could improve
this situation.

Furthermore, there are smaller technical issues that could be looked into
in order to increase the technological reach of the software. One thing
we noticed is that the exception handling of the R language does not work
correctly when being wrapped in calls to `future_lapply` from the
`future.apply`[@libfuture.apply] package, which is why we had to use
serial counterparts of that function for code that should be parallelizable.
Finding solutions for this issue could greatly cut down the time required
at least for the data acquisition part. Also, it turned out that
the `saveRDS` function we needed to persistently store results of expensive
computations like the LDA model training can require considerably higher
amounts of memory for serializing data structures that fit well into memory
for a long time before the serialization takes place. If this situation could
be improved, larger LDA models could be trained within existing memory
constraints.

## More appropriate visualizations

It could be argued that while there may be topics that are geographic by
their nature, thereby making maps or cartograms a fitting visualization,
this is not really insightful for topics that do not have a connection to
the countries' geography at all. For example, for the tax-related treaties
of a country, issues like the possibly nested interdependence of companies
across national borders is of greater importance than the size or location
of the involved countries.

An interesting option for future efforts may be network-style visualizations,
although this might require more data cleanup or coverage of more
countries' parliaments.

# Acknowledgements

We are thankful to Dr. Thomas Köntges who did not only provide us a great
introduction to available methods before project start, but also with valuable
pointers for handling the challenges we encountered throughout the project.

Computations for this work were done in part using resources of the
Leipzig University Computing Centre. Therefore, we are also thankful for
getting computing access there.

# Remarks

# References

TODO: e.g. websites of various parliaments, research papers we read on the topic, libraries we used for the analysis etc.
